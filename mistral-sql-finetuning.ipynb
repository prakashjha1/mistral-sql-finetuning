{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI9XuEjLfumT"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers peft accelerate bitsandbytes datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import all the essential library\n",
        "from datasets import load_dataset\n",
        "import collections\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "A_pTKEL4gDoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to huggingface by using the access token\n",
        "HF_TOKEN = userdata.get('hf_access')\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "vXCHFbXLujoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and print sql dataset\n",
        "dataset_name = \"b-mc2/sql-create-context\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "print(dataset[:5])\n"
      ],
      "metadata": {
        "id": "QLmNPc6JhS-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(example):\n",
        "    # Define Prompt template\n",
        "    prompt_template = \"\"\"### INSTRUCTION:\n",
        "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given the CREATE TABLE statement for the database and a question. You must generate the corresponding SQL query.\n",
        "\n",
        "### DATABASE SCHEMA:\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\n",
        "### RESPONSE (SQL Query):\n",
        "{answer}\"\"\"\n",
        "\n",
        "    prompt = prompt_template.format(\n",
        "        context=example['context'],\n",
        "        question=example['question'],\n",
        "        answer=example['answer']\n",
        "    )\n",
        "\n",
        "\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "\n",
        "formatted_dataset = dataset.map(create_prompt)\n",
        "\n",
        "print(formatted_dataset[0]['text'])"
      ],
      "metadata": {
        "id": "PJ3MhthBiVT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset)"
      ],
      "metadata": {
        "id": "BjQEhsgmnHsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the token or input length\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def getTokenLength(example):\n",
        "    return {\"token_length\": len(tokenizer.encode(example['text']))}\n",
        "\n",
        "dataset_with_lengths = formatted_dataset.map(getTokenLength)"
      ],
      "metadata": {
        "id": "0oJxPyi9ouul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary  of token length\n",
        "lengths_df = pd.DataFrame(dataset_with_lengths['token_length'], columns=['Token Length'])\n",
        "print(lengths_df.describe())"
      ],
      "metadata": {
        "id": "vzgI0BTxrlVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql_keywords = [\"SELECT\", \"FROM\", \"WHERE\", \"GROUP BY\", \"ORDER BY\", \"LIMIT\", \"JOIN\", \"ON\", \"AS\", \"DISTINCT\"]\n",
        "keyword_counts = collections.defaultdict(int)\n",
        "\n",
        "\n",
        "for query in dataset['answer']:\n",
        "\n",
        "    query_upper = query.upper()\n",
        "    for keyword in sql_keywords:\n",
        "        if keyword in query_upper:\n",
        "            keyword_counts[keyword] += 1\n",
        "\n",
        "print(\"Frequency of SQL Keywords in Answers:\")\n",
        "\n",
        "for keyword, count in sorted(keyword_counts.items(), key=lambda item: item[1], reverse=True):\n",
        "    print(f\"{keyword}: {count}\")"
      ],
      "metadata": {
        "id": "ml6y6cv7vJu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "SaLDZXQ4vkt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512\n",
        "def tokenize_and_prepare_labels(example):\n",
        "    # Take the complete format\n",
        "    full_prompt = example['text']\n",
        "\n",
        "    # Tokenize it. truncation=True mean if prompt is very large the remove or cut extra part\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, truncation=True, max_length=MAX_LENGTH, padding='max_length')\n",
        "\n",
        "    # make a copy of input ids\n",
        "    labels = tokenized_full_prompt['input_ids'].copy()\n",
        "\n",
        "    user_prompt = f\"\"\"### INSTRUCTION:\n",
        "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given the CREATE TABLE statement for the database and a question. You must generate the corresponding SQL query.\n",
        "\n",
        "### DATABASE SCHEMA:\n",
        "{example['context']}\n",
        "\n",
        "### QUESTION:\n",
        "{example['question']}\n",
        "\n",
        "### RESPONSE (SQL Query):\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize the input part\n",
        "    tokenized_user_prompt = tokenizer(user_prompt, truncation=True, max_length=MAX_LENGTH, padding=False)\n",
        "\n",
        "    # Find the length of input part\n",
        "    user_prompt_len = len(tokenized_user_prompt['input_ids'])\n",
        "\n",
        "\n",
        "    # mask the input part using -100 so that loss will be calculate using answer\n",
        "    for i in range(user_prompt_len):\n",
        "        labels[i] = -100\n",
        "\n",
        "    # put label and input in final dictionary\n",
        "    tokenized_full_prompt['labels'] = labels\n",
        "\n",
        "    return tokenized_full_prompt\n",
        "\n",
        "# Apply above function on complete dataset\n",
        "tokenized_dataset = formatted_dataset.map(tokenize_and_prepare_labels, batched=False)\n",
        "\n",
        "# Lets check an example\n",
        "print(tokenized_dataset[0]['input_ids'])\n",
        "print(tokenized_dataset[0]['labels'])"
      ],
      "metadata": {
        "id": "aw5VhbaD7b_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model name\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load the model in 4-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",  # it will automatically put the model on GPU\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ZKwEjl1T7hRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for PEFT training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Make LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # Rank of the update matrices. Higher = more parameters, but might overfit. 16 is a good starting point.\n",
        "    lora_alpha=32, # Alpha scaling factor. A good rule of thumb is alpha = 2 * r.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # where to add LoRA adapters inside the model's layer. This is standard for mistral.\n",
        "    lora_dropout=0.05, # Dropout probability for LoRA layers to prevent overfitting.\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\", # We are doing Causal Language Modeling (predicting the next token).\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA config\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Let's see the trainable parameters\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "x-un_lo_-cku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sql-finetune-results\",   # where to save results (checkpoints)\n",
        "    num_train_epochs=1,  # Number of epochs. 1 is a good start for large datasets.\n",
        "    per_device_train_batch_size=4, # how many examples will be given to GPU at a time\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_8bit\", # Memory-efficient optimizer\n",
        "    learning_rate=2e-4, # How fast the model learns. 2e-4 is a good value for LoRA.\n",
        "    lr_scheduler_type=\"cosine\", # schedule to make learning rate low\n",
        "    save_strategy=\"epoch\", # save model after each epoch\n",
        "    logging_steps=100, # Print loss on every 100 steps\n",
        "    report_to=\"none\", # We don't want to log to wandb or other services for now\n",
        ")"
      ],
      "metadata": {
        "id": "VSABm-m7_Q5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "# train_subset = tokenized_dataset.select(range(1000)) # uncomment this line if you want to use only 1000 examples else use data_collector\n",
        "# Create a Trainer Object\n",
        "trainer = Trainer(\n",
        "    model=peft_model,  # Our trainable LoRA model\n",
        "    train_dataset=train_subset, # Our tokenized dataset\n",
        "    args=training_args, # Training arguments which we have define earlier\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Let's gooooo!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "kr2mkXMOA1Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_history = trainer.state.log_history\n",
        "\n",
        "steps = [entry['step'] for entry in training_history if 'loss' in entry]\n",
        "losses = [entry['loss'] for entry in training_history if 'loss' in entry]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, losses, label=\"Training Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vsqoZECfA8CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"mistral-7b-sql-finetuned\"\n",
        "\n",
        "peft_model.save_pretrained(output_dir) # save finetunned model\n",
        "\n",
        "tokenizer.save_pretrained(output_dir) # Save Tokenizer"
      ],
      "metadata": {
        "id": "QPvMEVmY4Ybz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Database schema (context) for a new question\n",
        "test_context = \"\"\"CREATE TABLE head (age INTEGER)\"\"\"\n",
        "\n",
        "# Our new question\n",
        "test_question = \"How many heads of the departments are older than 56?\"\n",
        "\n",
        "\n",
        "# Use the same prompt template which was used in training but don't pass answer's part\n",
        "prompt = f\"\"\"### INSTRUCTION:\n",
        "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given the CREATE TABLE statement for the database and a question. You must generate the corresponding SQL query.\n",
        "\n",
        "### DATABASE SCHEMA:\n",
        "{test_context}\n",
        "\n",
        "### QUESTION:\n",
        "{test_question}\n",
        "\n",
        "### RESPONSE (SQL Query):\n",
        "\"\"\"\n",
        "\n",
        "# Using pipeline is the easy way to use text generateion\n",
        "# peft_model is the model which we finetunned\n",
        "text_gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=peft_model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# do_sample=True adds some randomness for better results\n",
        "sequences = text_gen_pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "\n",
        "full_response = sequences[0]['generated_text']\n",
        "sql_query = full_response.split(\"### RESPONSE (SQL Query):\")[1].strip()\n",
        "\n",
        "print(\"---PROMPT---\")\n",
        "print(prompt)\n",
        "print(\"\\n---GENERATED SQL---\")\n",
        "print(sql_query)"
      ],
      "metadata": {
        "id": "IKgEJIxg4-7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5NUQIhH5w7c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}